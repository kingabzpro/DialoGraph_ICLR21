{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nlp_en = spacy.load('en')\n",
    "nlp_sp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿ tienes alguna etiqueta láser o español ? \n"
     ]
    }
   ],
   "source": [
    "def read_tables(fname, falign):\n",
    "    # with open(\"data/chat_prev/en_rule.txt\") as f:\n",
    "    #     en_orig = [line.replace('\\n','') for line in f.readlines()]\n",
    "    # with open(\"data/chat_prev/sp_rule.txt\") as f:\n",
    "    #     sp_orig = [line.replace('\\n','') for line in f.readlines()]\n",
    "\n",
    "    en_orig = []\n",
    "    sp_orig = []\n",
    "    with open(fname) as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.replace('\\n','').split(' ||| ')\n",
    "            en_orig.append(items[0])\n",
    "            sp_orig.append(items[1])\n",
    "            \n",
    "    en_idx = []\n",
    "    sp_idx = []\n",
    "    with open(falign) as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.replace('\\n','').split()\n",
    "            en_idx.append([int(pair.split('-')[0]) for pair in items])\n",
    "            sp_idx.append([int(pair.split('-')[1]) for pair in items])\n",
    "        \n",
    "    return dict([('en', en_orig), ('sp', sp_orig), ('en_idx', en_idx), ('sp_idx', sp_idx)])\n",
    "\n",
    "tables_dct = read_tables(\"en-sp_rule.txt\", \"aligned.gdfa\")\n",
    "print tables_dct['sp'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sp = \"me gusta nadar. yo soy una estudiante\"\n",
    "sent_en = \"I like swimming and i study islamic studies\"\n",
    "doc_sp = nlp_sp(unicode(sent_sp, \"utf-8\"))\n",
    "doc_en = nlp_en(unicode(sent_en, \"utf-8\"))\n",
    "\n",
    "# chunks = list(doc_sp.noun_chunks)\n",
    "# print [chunk.text for chunk in chunks]\n",
    "\n",
    "# print [(en_token.text, en_token.pos_) for en_token in doc_en]\n",
    "# print [(en_token.text, en_token.pos_) for en_token in doc_sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f167efd44b6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msp_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables_dct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0men_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_txt' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    en_txt = tables_dct['en'][i]\n",
    "    sp_txt = tables_dct['sp'][i]\n",
    "    en_words = en_txt.split()\n",
    "    sp_words = sp_txt.split()\n",
    "    en_idxs = tables_dct['en_idx'][i]\n",
    "    sp_idxs = tables_dct['sp_idx'][i]\n",
    "    \n",
    "    en_doc = nlp_en(unicode(en_txt, \"utf-8\"))\n",
    "    sp_doc = nlp_sp(unicode(sp_txt, \"utf-8\"))\n",
    "    \n",
    "    for noun_chunk in en_doc.noun_chunks: # noun_chunk type = Span\n",
    "        chunk_start = noun_chunk.start\n",
    "        chunk_end = noun_chunk.end\n",
    "        \n",
    "        \n",
    "    \n",
    "    for j, e_idx in enumerate(tables_dct['en_idx'][i]):\n",
    "        s_idx = s_idxs[j]\n",
    "#         print en_words[e_idx], sp_words[s_idx]\n",
    "#     print \"*\"*10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "# displacy.serve(doc_sp, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_matrix_eng_nouns(en_txt, idx): #tables_dct, \n",
    "    \"\"\" en_txt: string of words (not list)\n",
    "        tables_dct: contains 2 tables, keys = en,sp\n",
    "        idx: int of which index in table is this sentence\n",
    "        [return] string of words (list separated by space)\n",
    "    \"\"\"\n",
    "    en_txt = tables_dct['en'][idx]\n",
    "    en_doc = nlp_en(unicode(en_txt, \"utf-8\"))\n",
    "#     en_nouns = [en_token.text for en_token in en_doc if en_token.pos_ in ['NOUN', 'PROPN']]\n",
    "    # avoid \"who\"\n",
    "#     en_nouns = [noun for noun in en_nouns if not noun==u\"who\"]\n",
    "    # print \"EN NOUNS\", en_nouns\n",
    "\n",
    "    sp_txt = tables_dct['sp'][idx]\n",
    "    sp_doc = nlp_sp(unicode(sp_txt, \"utf-8\"))\n",
    "    sp_nouns = []\n",
    "    for orig_i, sp_token in enumerate(sp_doc):\n",
    "        if sp_token.pos_ in ['NOUN', 'PROPN']: sp_nouns.append((sp_token.text, orig_i))\n",
    "        # capture infinitive verbs ==> turn into noun (e.g. nadar = swimming)\n",
    "        if sp_token.pos_=='VERB' and sp_token.tag_.endswith('Inf'): sp_nouns.append((sp_token.text, orig_i))\n",
    "        \n",
    "    # print \"SP NOUNS\", sp_nouns\n",
    "\n",
    "    cm_txt = [sp_token.text for sp_token in sp_doc]\n",
    "    for noun_idx, (sp_token, orig_i) in enumerate(sp_nouns):\n",
    "        if not en_nouns: break\n",
    "        if noun_idx < len(en_nouns)-1: #valid, or else is last\n",
    "            cm_txt[orig_i] = en_nouns[noun_idx]\n",
    "        else: # get last noun (probably last one anyway??)\n",
    "            cm_txt[orig_i] = en_nouns[-1]\n",
    "    return \" \".join(cm_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_sp_matrix_eng_nouns(en_txt, idx): #tables_dct, \n",
    "    \"\"\" en_txt: string of words (not list)\n",
    "        tables_dct: contains 2 tables, keys = en,sp\n",
    "        idx: int of which index in table is this sentence\n",
    "        [return] string of words (list separated by space)\n",
    "    \"\"\"\n",
    "    en_doc = nlp_en(unicode(en_txt, \"utf-8\"))\n",
    "    en_nouns = [en_token.text for en_token in en_doc if en_token.pos_ in ['NOUN', 'PROPN']]\n",
    "    # avoid \"who\"\n",
    "    en_nouns = [noun for noun in en_nouns if not noun==u\"who\"]\n",
    "    # print \"EN NOUNS\", en_nouns\n",
    "\n",
    "    sp_txt = tables_dct['sp'][idx]\n",
    "    sp_doc = nlp_sp(unicode(sp_txt, \"utf-8\"))\n",
    "    sp_nouns = []\n",
    "    for orig_i, sp_token in enumerate(sp_doc):\n",
    "        if sp_token.pos_ in ['NOUN', 'PROPN']: sp_nouns.append((sp_token.text, orig_i))\n",
    "        # capture infinitive verbs ==> turn into noun (e.g. nadar = swimming)\n",
    "        if sp_token.pos_=='VERB' and sp_token.tag_.endswith('Inf'): sp_nouns.append((sp_token.text, orig_i))\n",
    "        \n",
    "    # print \"SP NOUNS\", sp_nouns\n",
    "\n",
    "    cm_txt = [sp_token.text for sp_token in sp_doc]\n",
    "    for noun_idx, (sp_token, orig_i) in enumerate(sp_nouns):\n",
    "        if not en_nouns: break\n",
    "        if noun_idx < len(en_nouns)-1: #valid, or else is last\n",
    "            cm_txt[orig_i] = en_nouns[noun_idx]\n",
    "        else: # get last noun (probably last one anyway??)\n",
    "            cm_txt[orig_i] = en_nouns[-1]\n",
    "    return \" \".join(cm_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sp = \"Tengo dos amigos que estudiaron estudios islámicos.\"\n",
    "sent_en = \"I have two friends who studied islamic studies.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lex():\n",
    "\tprint \"STYLE 1: LEXICAL on Rule-based Bot // 6 Feb 2018\"\n",
    "\tprint \"KEY:\\n1. Spanish (orig)\\n2. English (orig)\\n3. CodeMix\"\n",
    "\tprint \"*\"*10\n",
    "\n",
    "\tfor i, en_sent in enumerate(tables_dct['en']):\n",
    "\t\tprint tables_dct['sp'][i]\n",
    "\t\tprint en_sent\n",
    "\t\tprint sp_matrix_eng_nouns(en_sent, i).encode('utf-8') # encoding req'd to save print logs\n",
    "\t\tprint \"*\"*10\n",
    "\t\t# if i==10: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
